{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yonad91/-Plant-Disease-Detection/blob/main/PlantDiseaseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (TensorFlow, Gradio for potential later use)\n",
        "!pip install tensorflow gradio kaggle -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "KAGGLE_DATASET = 'vipoooool/new-plant-diseases-dataset'\n",
        "DATASET_PATH = '/content/PlantDiseases'\n",
        "TRAIN_DIR = os.path.join(DATASET_PATH, 'train')\n",
        "VALID_DIR = os.path.join(DATASET_PATH, 'valid')\n",
        "\n",
        "# --- 0. Clean up previous environment ---\n",
        "print(\"0. Cleaning up previous environment...\")\n",
        "if os.path.exists(os.path.expanduser('~/.kaggle')):\n",
        "    shutil.rmtree(os.path.expanduser('~/.kaggle'))\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    shutil.rmtree(DATASET_PATH)\n",
        "if os.path.exists('/content/New Plant Diseases Dataset(Augmented)'):\n",
        "    shutil.rmtree('/content/New Plant Diseases Dataset(Augmented)')\n",
        "print(\"Cleanup complete.\")\n",
        "\n",
        "# --- 1. Configure Kaggle API and Download ---\n",
        "print(\"\\n1. Configuring Kaggle API and Downloading Data...\")\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Please upload your 'kaggle.json' file when prompted.\")\n",
        "\n",
        "    # This line initiates the file browsing dialog.\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        uploaded_filename = list(uploaded.keys())[0]\n",
        "        uploaded_path = os.path.join('/content/', uploaded_filename)\n",
        "\n",
        "        # Configure API\n",
        "        !mkdir -p ~/.kaggle\n",
        "        # FIX: Wrapping the uploaded_path in quotes ensures that filenames with spaces or parentheses (like 'kaggle (2).json') are handled correctly by the shell.\n",
        "        !cp \"{uploaded_path}\" ~/.kaggle/kaggle.json\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "        print(\"Kaggle configuration successful.\")\n",
        "\n",
        "        # Download\n",
        "        print(\"Downloading dataset (may take a few minutes)...\")\n",
        "        !kaggle datasets download -d {KAGGLE_DATASET} -p /content/\n",
        "\n",
        "        # Find and extract zip file\n",
        "        zip_files = [f for f in os.listdir('/content') if f.endswith('.zip')]\n",
        "\n",
        "        if zip_files:\n",
        "            zip_file_path = os.path.join('/content', zip_files[0])\n",
        "            print(f\"Found zip file: {zip_files[0]}. Extracting...\")\n",
        "\n",
        "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content/')\n",
        "\n",
        "            # --- FIXED DATA ORGANIZATION ---\n",
        "            SOURCE_ROOT = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)'\n",
        "\n",
        "            if os.path.exists(os.path.join(SOURCE_ROOT, 'train')):\n",
        "                os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "                shutil.move(os.path.join(SOURCE_ROOT,'train'), TRAIN_DIR)\n",
        "                shutil.move(os.path.join(SOURCE_ROOT,'valid'), VALID_DIR)\n",
        "\n",
        "                # Cleanup auxiliary folders\n",
        "                shutil.rmtree('/content/New Plant Diseases Dataset(Augmented)', ignore_errors=True)\n",
        "                os.remove(zip_file_path)\n",
        "                print(\"Dataset successfully extracted and organized.\")\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"Extraction failed: Expected source folder not found at {SOURCE_ROOT}.\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Dataset zip file not found after download attempt.\")\n",
        "\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Kaggle API key is missing. Cannot proceed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during setup: {e}\")\n",
        "    if not os.path.exists(TRAIN_DIR):\n",
        "        raise FileNotFoundError(f\"FATAL: Data not found in {TRAIN_DIR}. Cannot proceed.\")\n",
        "\n",
        "# --- 2. Define Hyperparameters and Data Generators ---\n",
        "if not os.path.exists(TRAIN_DIR):\n",
        "    raise FileNotFoundError(f\"Training directory not found: {TRAIN_DIR}. Data extraction failed.\")\n",
        "\n",
        "OUTPUT_SHAPE = len(os.listdir(TRAIN_DIR))\n",
        "\n",
        "# Data Augmentation for Training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, rotation_range=20, width_shift_range=0.1,\n",
        "    height_shift_range=0.1, shear_range=0.1, zoom_range=0.1,\n",
        "    horizontal_flip=True, fill_mode='nearest'\n",
        ")\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create Data Generators\n",
        "print(\"\\nCreating data generators...\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True\n",
        ")\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    VALID_DIR, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
        "print(f\"\\nConfiguration complete. Classes detected: {OUTPUT_SHAPE}.\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "Bm8fClxouiUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Load Pre-trained VGG16 Base Model ---\n",
        "print(\"3. Loading VGG16 base model (pre-trained on ImageNet)...\")\n",
        "\n",
        "# Instantiate the VGG16 model, excluding the top (classification) layer\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False, # Important: Exclude the original classification head\n",
        "    input_tensor=Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        ")\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "# This prevents the VGG16 weights from being updated during initial training.\n",
        "base_model.trainable = False\n",
        "print(\"VGG16 base model layers are frozen and will act as a feature extractor.\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "vXWjinX9umJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Build the Transfer Learning Model ---\n",
        "print(\"4. Building the custom classification model head...\")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Add the frozen VGG16 base model\n",
        "    base_model,\n",
        "\n",
        "    # Global Average Pooling 2D layer reduces the 7x7x512 feature maps to 512 features\n",
        "    GlobalAveragePooling2D(),\n",
        "\n",
        "    # Add a Dense layer for feature refinement and non-linearity\n",
        "    Dense(256, activation='relu'),\n",
        "\n",
        "    # Add a Dropout layer for regularization (prevents overfitting)\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output layer with 'OUTPUT_SHAPE' (38) units and softmax for multi-class classification\n",
        "    Dense(OUTPUT_SHAPE, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"Model architecture defined.\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "7fpdpCN_uq-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Compile the Model ---\n",
        "print(\"5. Compiling the model...\")\n",
        "\n",
        "# Use the Adam optimizer with a low learning rate for stable convergence\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy', # Used for multi-class classification\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model summary to confirm architecture and trainable parameters\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "print(f\"Total parameters: {model.count_params()}\")\n",
        "print(f\"Trainable parameters (only the new head): {len(model.trainable_weights)}\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "Uwt-te2Jut6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Train the Model ---\n",
        "print(\"6. Starting initial model training (Training only the top classification layers)...\")\n",
        "\n",
        "# Define a moderate number of epochs for initial training\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=valid_generator.samples // BATCH_SIZE\n",
        ")\n",
        "\n",
        "print(\"\\nInitial training complete.\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "tkc4US1Euw_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Evaluation and Visualization ---\n",
        "print(\"7. Evaluating and visualizing training history...\")\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots the training and validation accuracy and loss.\"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Execute plotting\n",
        "plot_history(history)\n",
        "\n",
        "print(\"\\nVisualization complete. Model training concluded.\")\n",
        "\n",
        "# Optional: Save the model\n",
        "model.save('plant_disease_vgg16_frozen.h5')\n",
        "print(\"Model saved as plant_disease_vgg16_frozen.h5\")\n",
        "print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "EFZog0hOIYQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPk/TgakWzni7SFNrH68RGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}